{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\"AXP\",\"AAPL\",\"BA\",\"CAT\",\"CVX\",\"CSCO\",\"DIS\",\"DOW\", \"XOM\",\n",
    "           \"HD\",\"IBM\",\"INTC\",\"JNJ\",\"KO\",\"MCD\",\"MMM\",\"MRK\",\"MSFT\",\n",
    "           \"NKE\",\"PFE\",\"PG\",\"TRV\",\"UTX\",\"UNH\",\"VZ\",\"V\",\"WMT\",\"WBA\"]\n",
    "\n",
    "#list of tickers whose financial data needs to be extracted\n",
    "financial_dir = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tickers:\n",
    "    try:\n",
    "    #getting balance sheet data from yahoo finance for the given ticker\n",
    "        temp_dir = {}\n",
    "        url = 'https://in.finance.yahoo.com/quote/'+ticker+'/balance-sheet?p='+ticker\n",
    "        page = requests.get(url)\n",
    "        page_content = page.content\n",
    "        soup = BeautifulSoup(page_content,'html.parser')\n",
    "        tabl = soup.find_all(\"div\", {\"class\" : \"M(0) Whs(n) BdEnd Bdc($seperatorColor) D(itb)\"})\n",
    "        for t in tabl:\n",
    "            rows = t.find_all(\"div\", {\"class\" : \"rw-expnded\"})\n",
    "            for row in rows:\n",
    "                temp_dir[row.get_text(separator='|').split(\"|\")[0]]=row.get_text(separator='|').split(\"|\")[1]\n",
    "        \n",
    "        #getting income statement data from yahoo finance for the given ticker\n",
    "        url = 'https://in.finance.yahoo.com/quote/'+ticker+'/financials?p='+ticker\n",
    "        page = requests.get(url)\n",
    "        page_content = page.content\n",
    "        soup = BeautifulSoup(page_content,'html.parser')\n",
    "        tabl = soup.find_all(\"div\", {\"class\" : \"M(0) Whs(n) BdEnd Bdc($seperatorColor) D(itb)\"})\n",
    "        for t in tabl:\n",
    "            rows = t.find_all(\"div\", {\"class\" : \"rw-expnded\"})\n",
    "            for row in rows:\n",
    "                temp_dir[row.get_text(separator='|').split(\"|\")[0]]=row.get_text(separator='|').split(\"|\")[1]\n",
    "        \n",
    "        #getting cashflow statement data from yahoo finance for the given ticker\n",
    "        url = 'https://in.finance.yahoo.com/quote/'+ticker+'/cash-flow?p='+ticker\n",
    "        page = requests.get(url)\n",
    "        page_content = page.content\n",
    "        soup = BeautifulSoup(page_content,'html.parser')\n",
    "        tabl = soup.find_all(\"div\", {\"class\" : \"M(0) Whs(n) BdEnd Bdc($seperatorColor) D(itb)\"})\n",
    "        for t in tabl:\n",
    "            rows = t.find_all(\"div\", {\"class\" : \"rw-expnded\"})\n",
    "            for row in rows:\n",
    "                temp_dir[row.get_text(separator='|').split(\"|\")[0]]=row.get_text(separator='|').split(\"|\")[1]\n",
    "        \n",
    "        #getting key statistics data from yahoo finance for the given ticker\n",
    "        url = 'https://in.finance.yahoo.com/quote/'+ticker+'/key-statistics?p='+ticker\n",
    "        page = requests.get(url)\n",
    "        page_content = page.content\n",
    "        soup = BeautifulSoup(page_content,'html.parser')\n",
    "        tabl = soup.findAll(\"table\", {\"class\": \"W(100%) Bdcl(c) \"}) # try soup.findAll(\"table\") if this line gives error \n",
    "        for t in tabl:\n",
    "            rows = t.find_all(\"tr\")\n",
    "            for row in rows:\n",
    "                if len(row.get_text(separator='|').split(\"|\")[0:2])>0:\n",
    "                    temp_dir[row.get_text(separator='|').split(\"|\")[0]]=row.get_text(separator='|').split(\"|\")[-1]    \n",
    "        \n",
    "        #combining all extracted information with the corresponding ticker\n",
    "        financial_dir[ticker] = temp_dir\n",
    "    except:\n",
    "        print(\"Problem scraping data for \",ticker)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing information in pandas dataframe\n",
    "combined_financials = pd.DataFrame(financial_dir)\n",
    "combined_financials.dropna(how='all',axis=1,inplace=True) #dropping columns with all NaN values\n",
    "tickers = combined_financials.columns #updating the tickers list based on only those tickers whose values were successfully extracted\n",
    "for ticker in tickers:\n",
    "    combined_financials = combined_financials[~combined_financials[ticker].str.contains(\"[a-z]\").fillna(False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't read data for  AXP\n",
      "can't read data for  AAPL\n",
      "can't read data for  BA\n",
      "can't read data for  CAT\n",
      "can't read data for  CVX\n",
      "can't read data for  CSCO\n",
      "can't read data for  DIS\n",
      "can't read data for  DOW\n",
      "can't read data for  XOM\n",
      "can't read data for  HD\n",
      "can't read data for  IBM\n",
      "can't read data for  INTC\n",
      "can't read data for  JNJ\n",
      "can't read data for  KO\n",
      "can't read data for  MCD\n",
      "can't read data for  MMM\n",
      "can't read data for  MRK\n",
      "can't read data for  MSFT\n",
      "can't read data for  NKE\n",
      "can't read data for  PFE\n",
      "can't read data for  PG\n",
      "can't read data for  TRV\n",
      "can't read data for  UNH\n",
      "can't read data for  VZ\n",
      "can't read data for  V\n",
      "can't read data for  WMT\n"
     ]
    }
   ],
   "source": [
    "# creating dataframe with relevant financial information for each stock using fundamental data\n",
    "stats = [\"EBITDA\",\n",
    "         \"Depreciation & amortisation\",\n",
    "         \"Market cap (intra-day)\",\n",
    "         \"Net income available to common shareholders\",\n",
    "         \"Net cash provided by operating activities\",\n",
    "         \"Capital expenditure\",\n",
    "         \"Total current assets\",\n",
    "         \"Total current liabilities\",\n",
    "         \"Net property, plant and equipment\",\n",
    "         \"Total stockholders' equity\",\n",
    "         \"Long-term debt\",\n",
    "         \"Forward annual dividend yield\"] # change as required\n",
    "\n",
    "indx = [\"EBITDA\",\"D&A\",\"MarketCap\",\"NetIncome\",\"CashFlowOps\",\"Capex\",\"CurrAsset\",\n",
    "        \"CurrLiab\",\"PPE\",\"BookValue\",\"TotDebt\",\"DivYield\"]\n",
    "all_stats = {}\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        temp = combined_financials[ticker]\n",
    "        ticker_stats = []\n",
    "        for stat in stats:\n",
    "            ticker_stats.append(temp.loc[stat])\n",
    "        all_stats['{}'.format(ticker)] = ticker_stats\n",
    "    except:\n",
    "        print(\"can't read data for \",ticker)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['AXP', 'AAPL', 'BA', 'CAT', 'CVX', 'CSCO', 'DIS', 'DOW', 'XOM', 'HD',\\n       'IBM', 'INTC', 'JNJ', 'KO', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PFE',\\n       'PG', 'TRV', 'UNH', 'VZ', 'V', 'WMT'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-df6ebccfa552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# cleansing of fundamental data imported in dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_stats_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_stats_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_stats_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mall_stats_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_stats_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'M'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'E+03'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_stats_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_stats_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'E+06'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2984\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2986\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"raise_missing\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         )\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 raise KeyError(\n\u001b[1;32m   1176\u001b[0m                     \"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1177\u001b[0;31m                         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m                     )\n\u001b[1;32m   1179\u001b[0m                 )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['AXP', 'AAPL', 'BA', 'CAT', 'CVX', 'CSCO', 'DIS', 'DOW', 'XOM', 'HD',\\n       'IBM', 'INTC', 'JNJ', 'KO', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PFE',\\n       'PG', 'TRV', 'UNH', 'VZ', 'V', 'WMT'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# cleansing of fundamental data imported in dataframe\n",
    "all_stats_df = pd.DataFrame(all_stats,index=indx)\n",
    "all_stats_df[tickers] = all_stats_df[tickers].replace({',': ''}, regex=True)\n",
    "all_stats_df[tickers] = all_stats_df[tickers].replace({'M': 'E+03'}, regex=True)\n",
    "all_stats_df[tickers] = all_stats_df[tickers].replace({'B': 'E+06'}, regex=True)\n",
    "all_stats_df[tickers] = all_stats_df[tickers].replace({'T': 'E+09'}, regex=True)\n",
    "all_stats_df[tickers] = all_stats_df[tickers].replace({'%': 'E-02'}, regex=True)\n",
    "for ticker in all_stats_df.columns:\n",
    "    all_stats_df[ticker] = pd.to_numeric(all_stats_df[ticker].values,errors='coerce')\n",
    "all_stats_df.dropna(axis=1,inplace=True)\n",
    "tickers = all_stats_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating relevant financial metrics for each stock\n",
    "transpose_df = all_stats_df.transpose()\n",
    "final_stats_df = pd.DataFrame()\n",
    "final_stats_df[\"EBIT\"] = transpose_df[\"EBIT\"]\n",
    "final_stats_df[\"TEV\"] =  transpose_df[\"MarketCap\"].fillna(0) \\\n",
    "                         +transpose_df[\"TotDebt\"].fillna(0) \\\n",
    "                         +transpose_df[\"PrefStock\"].fillna(0) \\\n",
    "                         +transpose_df[\"MinInterest\"].fillna(0) \\\n",
    "                         -(transpose_df[\"CurrAsset\"].fillna(0)-transpose_df[\"CurrLiab\"].fillna(0))\n",
    "final_stats_df[\"EarningYield\"] =  final_stats_df[\"EBIT\"]/final_stats_df[\"TEV\"]\n",
    "final_stats_df[\"FCFYield\"] = (transpose_df[\"CashFlowOps\"]-transpose_df[\"Capex\"])/transpose_df[\"MarketCap\"]\n",
    "final_stats_df[\"ROC\"]  = transpose_df[\"EBIT\"]/(transpose_df[\"PPE\"]+transpose_df[\"CurrAsset\"]-transpose_df[\"CurrLiab\"])\n",
    "final_stats_df[\"BookToMkt\"] = transpose_df[\"BookValue\"]/transpose_df[\"MarketCap\"]\n",
    "final_stats_df[\"DivYield\"] = transpose_df[\"DivYield\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Value stocks based on Greenblatt's Magic Formula\n",
      "Empty DataFrame\n",
      "Columns: [EarningYield, ROC, MagicFormulaRank]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# finding value stocks based on Magic Formula\n",
    "final_stats_val_df = final_stats_df.loc[tickers,:]\n",
    "final_stats_val_df[\"CombRank\"] = final_stats_val_df[\"EarningYield\"].rank(ascending=False,na_option='bottom')+final_stats_val_df[\"ROC\"].rank(ascending=False,na_option='bottom')\n",
    "final_stats_val_df[\"MagicFormulaRank\"] = final_stats_val_df[\"CombRank\"].rank(method='first')\n",
    "value_stocks = final_stats_val_df.sort_values(\"MagicFormulaRank\").iloc[:,[2,4,8]]\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Value stocks based on Greenblatt's Magic Formula\")\n",
    "print(value_stocks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Highest dividend paying stocks\n",
      "Series([], Name: DivYield, dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "# finding highest dividend yield stocks\n",
    "high_dividend_stocks = final_stats_df.sort_values(\"DivYield\",ascending=False).iloc[:,6]\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Highest dividend paying stocks\")\n",
    "print(high_dividend_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Magic Formula and Dividend Yield combined\n",
      "Empty DataFrame\n",
      "Columns: [EarningYield, ROC, DivYield, CombinedRank]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# # Magic Formula & Dividend yield combined\n",
    "final_stats_df[\"CombRank\"] = final_stats_df[\"EarningYield\"].rank(ascending=False,method='first') \\\n",
    "                              +final_stats_df[\"ROC\"].rank(ascending=False,method='first')  \\\n",
    "                              +final_stats_df[\"DivYield\"].rank(ascending=False,method='first')\n",
    "final_stats_df[\"CombinedRank\"] = final_stats_df[\"CombRank\"].rank(method='first')\n",
    "value_high_div_stocks = final_stats_df.sort_values(\"CombinedRank\").iloc[:,[2,4,6,8]]\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Magic Formula and Dividend Yield combined\")\n",
    "print(value_high_div_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piotroski F score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping financial statement data for  AXP\n",
      "scraping financial statement data for  AAPL\n",
      "scraping financial statement data for  BA\n",
      "scraping financial statement data for  CAT\n",
      "scraping financial statement data for  CVX\n",
      "scraping financial statement data for  CSCO\n",
      "scraping financial statement data for  DIS\n",
      "scraping financial statement data for  DOW\n",
      "scraping financial statement data for  XOM\n",
      "scraping financial statement data for  HD\n",
      "scraping financial statement data for  IBM\n",
      "scraping financial statement data for  INTC\n",
      "scraping financial statement data for  JNJ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "tickers = [\"AXP\",\"AAPL\",\"BA\",\"CAT\",\"CVX\",\"CSCO\",\"DIS\",\"DOW\", \"XOM\",\n",
    "           \"HD\",\"IBM\",\"INTC\",\"JNJ\",\"KO\",\"MCD\",\"MMM\",\"MRK\",\"MSFT\",\n",
    "           \"NKE\",\"PFE\",\"PG\",\"TRV\",\"UTX\",\"UNH\",\"VZ\",\"V\",\"WMT\",\"WBA\"]\n",
    "\n",
    "#list of tickers whose financial data needs to be extracted\n",
    "financial_dir_cy = {} #directory to store current year's information\n",
    "financial_dir_py = {} #directory to store last year's information\n",
    "financial_dir_py2 = {} #directory to store last to last year's information\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        print(\"scraping financial statement data for \",ticker)\n",
    "        temp_dir = {}\n",
    "        temp_dir2 = {}\n",
    "        temp_dir3 = {}\n",
    "    #getting balance sheet data from yahoo finance for the given ticker\n",
    "        url = 'https://in.finance.yahoo.com/quote/'+ticker+'/balance-sheet?p='+ticker\n",
    "        page = requests.get(url)\n",
    "        page_content = page.content\n",
    "        soup = BeautifulSoup(page_content,'html.parser')\n",
    "        tabl = soup.find_all(\"div\", {\"class\" : \"M(0) Whs(n) BdEnd Bdc($seperatorColor) D(itb)\"})\n",
    "        for t in tabl:\n",
    "            rows = t.find_all(\"div\", {\"class\" : \"rw-expnded\"})\n",
    "            for row in rows:\n",
    "                temp_dir[row.get_text(separator='|').split(\"|\")[0]]=row.get_text(separator='|').split(\"|\")[1]\n",
    "                temp_dir2[row.get_text(separator='|').split(\"|\")[0]]=row.get_text(separator='|').split(\"|\")[2]\n",
    "                temp_dir3[row.get_text(separator='|').split(\"|\")[0]]=row.get_text(separator='|').split(\"|\")[3]\n",
    "        \n",
    "        #getting income statement data from yahoo finance for the given ticker\n",
    "        url = 'https://in.finance.yahoo.com/quote/'+ticker+'/financials?p='+ticker\n",
    "        page = requests.get(url)\n",
    "        page_content = page.content\n",
    "        soup = BeautifulSoup(page_content,'html.parser')\n",
    "        tabl = soup.find_all(\"div\", {\"class\" : \"M(0) Whs(n) BdEnd Bdc($seperatorColor) D(itb)\"})\n",
    "        for t in tabl:\n",
    "            rows = t.find_all(\"div\", {\"class\" : \"rw-expnded\"})\n",
    "            for row in rows:\n",
    "                temp_dir[row.get_text(separator='|').split(\"|\")[0]]=row.get_text(separator='|').split(\"|\")[1]\n",
    "                temp_dir2[row.get_text(separator='|').split(\"|\")[0]]=row.get_text(separator='|').split(\"|\")[2]\n",
    "                temp_dir3[row.get_text(separator='|').split(\"|\")[0]]=row.get_text(separator='|').split(\"|\")[3]\n",
    "        \n",
    "        #getting cashflow statement data from yahoo finance for the given ticker\n",
    "        url = 'https://in.finance.yahoo.com/quote/'+ticker+'/cash-flow?p='+ticker\n",
    "        page = requests.get(url)\n",
    "        page_content = page.content\n",
    "        soup = BeautifulSoup(page_content,'html.parser')\n",
    "        tabl = soup.find_all(\"div\", {\"class\" : \"M(0) Whs(n) BdEnd Bdc($seperatorColor) D(itb)\"})\n",
    "        for t in tabl:\n",
    "            rows = t.find_all(\"div\", {\"class\" : \"rw-expnded\"})\n",
    "            for row in rows:\n",
    "                temp_dir[row.get_text(separator='|').split(\"|\")[0]]=row.get_text(separator='|').split(\"|\")[1]\n",
    "                temp_dir2[row.get_text(separator='|').split(\"|\")[0]]=row.get_text(separator='|').split(\"|\")[2]\n",
    "                temp_dir3[row.get_text(separator='|').split(\"|\")[0]]=row.get_text(separator='|').split(\"|\")[3] \n",
    "        \n",
    "        #combining all extracted information with the corresponding ticker\n",
    "        financial_dir_cy[ticker] = temp_dir\n",
    "        financial_dir_py[ticker] = temp_dir2\n",
    "        financial_dir_py2[ticker] = temp_dir3\n",
    "    except:\n",
    "        print(\"Problem scraping data for \",ticker)\n",
    "\n",
    "\n",
    "#storing information in pandas dataframe\n",
    "combined_financials_cy = pd.DataFrame(financial_dir_cy)\n",
    "#combined_financials_cy.dropna(axis=1,inplace=True) #dropping columns with NaN values\n",
    "combined_financials_py = pd.DataFrame(financial_dir_py)\n",
    "#combined_financials_py.dropna(axis=1,inplace=True)\n",
    "combined_financials_py2 = pd.DataFrame(financial_dir_py2)\n",
    "#combined_financials_py2.dropna(axis=1,inplace=True)\n",
    "tickers = combined_financials_cy.columns #updating the tickers list based on only those tickers whose values were successfully extracted\n",
    "\n",
    "# selecting relevant financial information for each stock using fundamental data\n",
    "stats = [\"Net income available to common shareholders\",\n",
    "         \"Total assets\",\n",
    "         \"Net cash provided by operating activities\",\n",
    "         \"Long-term debt\",\n",
    "         \"Other long-term liabilities\",\n",
    "         \"Total current assets\",\n",
    "         \"Total current liabilities\",\n",
    "         \"Common stock\",\n",
    "         \"Total revenue\",\n",
    "         \"Gross profit\"] # change as required\n",
    "\n",
    "indx = [\"NetIncome\",\"TotAssets\",\"CashFlowOps\",\"LTDebt\",\"OtherLTDebt\",\n",
    "        \"CurrAssets\",\"CurrLiab\",\"CommStock\",\"TotRevenue\",\"GrossProfit\"]\n",
    "\n",
    "\n",
    "def info_filter(df,stats,indx):\n",
    "    \"\"\"function to filter relevant financial information for each \n",
    "       stock and transforming string inputs to numeric\"\"\"\n",
    "    tickers = df.columns\n",
    "    all_stats = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            temp = df[ticker]\n",
    "            ticker_stats = []\n",
    "            for stat in stats:\n",
    "                ticker_stats.append(temp.loc[stat])\n",
    "            all_stats['{}'.format(ticker)] = ticker_stats\n",
    "        except:\n",
    "            print(\"can't read data for \",ticker)\n",
    "    \n",
    "    all_stats_df = pd.DataFrame(all_stats,index=indx)\n",
    "    \n",
    "    # cleansing of fundamental data imported in dataframe\n",
    "    all_stats_df[tickers] = all_stats_df[tickers].replace({',': ''}, regex=True)\n",
    "    for ticker in all_stats_df.columns:\n",
    "        all_stats_df[ticker] = pd.to_numeric(all_stats_df[ticker].values,errors='coerce')\n",
    "    return all_stats_df\n",
    "\n",
    "def piotroski_f(df_cy,df_py,df_py2):\n",
    "    \"\"\"function to calculate f score of each stock and output information as dataframe\"\"\"\n",
    "    f_score = {}\n",
    "    tickers = df_cy.columns\n",
    "    for ticker in tickers:\n",
    "        ROA_FS = int(df_cy.loc[\"NetIncome\",ticker]/((df_cy.loc[\"TotAssets\",ticker]+df_py.loc[\"TotAssets\",ticker])/2) > 0)\n",
    "        CFO_FS = int(df_cy.loc[\"CashFlowOps\",ticker] > 0)\n",
    "        ROA_D_FS = int(df_cy.loc[\"NetIncome\",ticker]/(df_cy.loc[\"TotAssets\",ticker]+df_py.loc[\"TotAssets\",ticker])/2 > df_py.loc[\"NetIncome\",ticker]/(df_py.loc[\"TotAssets\",ticker]+df_py2.loc[\"TotAssets\",ticker])/2)\n",
    "        CFO_ROA_FS = int(df_cy.loc[\"CashFlowOps\",ticker]/df_cy.loc[\"TotAssets\",ticker] > df_cy.loc[\"NetIncome\",ticker]/((df_cy.loc[\"TotAssets\",ticker]+df_py.loc[\"TotAssets\",ticker])/2))\n",
    "        LTD_FS = int((df_cy.loc[\"LTDebt\",ticker] + df_cy.loc[\"OtherLTDebt\",ticker])<(df_py.loc[\"LTDebt\",ticker] + df_py.loc[\"OtherLTDebt\",ticker]))\n",
    "        CR_FS = int((df_cy.loc[\"CurrAssets\",ticker]/df_cy.loc[\"CurrLiab\",ticker])>(df_py.loc[\"CurrAssets\",ticker]/df_py.loc[\"CurrLiab\",ticker]))\n",
    "        DILUTION_FS = int(df_cy.loc[\"CommStock\",ticker] <= df_py.loc[\"CommStock\",ticker])\n",
    "        GM_FS = int((df_cy.loc[\"GrossProfit\",ticker]/df_cy.loc[\"TotRevenue\",ticker])>(df_py.loc[\"GrossProfit\",ticker]/df_py.loc[\"TotRevenue\",ticker]))\n",
    "        ATO_FS = int(df_cy.loc[\"TotRevenue\",ticker]/((df_cy.loc[\"TotAssets\",ticker]+df_py.loc[\"TotAssets\",ticker])/2)>df_py.loc[\"TotRevenue\",ticker]/((df_py.loc[\"TotAssets\",ticker]+df_py2.loc[\"TotAssets\",ticker])/2))\n",
    "        f_score[ticker] = [ROA_FS,CFO_FS,ROA_D_FS,CFO_ROA_FS,LTD_FS,CR_FS,DILUTION_FS,GM_FS,ATO_FS]\n",
    "    f_score_df = pd.DataFrame(f_score,index=[\"PosROA\",\"PosCFO\",\"ROAChange\",\"Accruals\",\"Leverage\",\"Liquidity\",\"Dilution\",\"GM\",\"ATO\"])\n",
    "    return f_score_df\n",
    "\n",
    "# Selecting stocks with highest Piotroski f score\n",
    "transformed_df_cy = info_filter(combined_financials_cy,stats,indx)\n",
    "transformed_df_py = info_filter(combined_financials_py,stats,indx)\n",
    "transformed_df_py2 = info_filter(combined_financials_py2,stats,indx)\n",
    "\n",
    "f_score_df = piotroski_f(transformed_df_cy,transformed_df_py,transformed_df_py2)\n",
    "f_score_df.sum().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
